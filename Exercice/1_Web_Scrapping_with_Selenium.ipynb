{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/kplr-training/Web-Scraping/blob/main/Exercice/1_Web_Scrapping_with_Selenium.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"amXTdSj4A-Rt"},"source":["# **Tutoriel : Extraire les articles d'une revue d'intellingence artificielle selon les catégories**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mRgVUOsvtoWG"},"source":["![image](https://user-images.githubusercontent.com/123748165/224306735-142be7ed-a969-458e-9bd3-65360d593a3e.png)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mSMgllBGu6K8"},"source":["- Afin de pouvoir utiliser Selenium il faut d’abord installer le Web Driver associé au navigateur que l’on souhaite utiliser.\n","\n","\n","- Vous devez ensuite installer Selenium et le Web Driver dans votre environnement.\n","\n","- Pour ce faire, on va utiliser dans notre notebook les commandes suivantes :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eLhu1CeVAiuT"},"outputs":[],"source":["!sudo add-apt-repository ppa:saiarcot895/chromium-beta\n","!sudo apt remove chromium-browser\n","!sudo snap remove chromium\n","!sudo apt install chromium-browser\n","!pip install selenium-stealth\n","!pip3 install selenium\n","!apt-get update\n","!apt install chromium-chromedriver\n","!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n","\n","import sys\n","sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')"]},{"cell_type":"markdown","metadata":{"id":"TrWyTNoAu9hj"},"source":["On importe les modules dont on aura besoin :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rrivkz3EAu6W"},"outputs":[],"source":["from selenium.webdriver.common.by import By\n","from selenium import webdriver\n","from selenium.webdriver.common.keys import Keys\n","from selenium.webdriver.support.ui import WebDriverWait\n","from selenium.webdriver.support import expected_conditions as EC\n","from time import sleep\n","from urllib.error import HTTPError\n","from time import time\n","from selenium.common.exceptions import NoSuchElementException\n","import pandas as pd\n","from selenium_stealth import stealth"]},{"cell_type":"markdown","metadata":{"id":"b0GiPLgLu_-5"},"source":["Enfin, on modifie quelques options du Chrome Driver pour pouvoir l’utiliser depuis un notebook :"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kk7-WVIsYTbo","outputId":"2e4c3796-bd6e-4b6c-c502-57cc7283746c"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-113-3a4dc5b0e958>:5: DeprecationWarning: use options instead of chrome_options\n","  wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n","<ipython-input-113-3a4dc5b0e958>:6: DeprecationWarning: use options instead of chrome_options\n","  driver =webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n"]}],"source":["chrome_options = webdriver.ChromeOptions()\n","chrome_options.add_argument('--headless')\n","chrome_options.add_argument('--no-sandbox')\n","chrome_options.add_argument('--disable-dev-shm-usage')\n","wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n","driver =webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n","stealth(driver,\n","              languages=[\"fr-FR\", \"fr\"],\n","              vendor=\"Google Inc.\",\n","              platform=\"Win32\",\n","              webgl_vendor=\"Intel Inc.\",\n","              renderer=\"Intel Iris OpenGL Engine\",\n","              fix_hairline=True,\n","          )"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"z1wk76ChvFtR"},"source":["# **Web Scraping des liens des articles**\n","Pour pouvoir faire correctement du Web Scraping, il faut se poser deux questions :\n","\n","- Comment est construit le site web ?\n","\n","- Quelles donnees souhaite-t-on recuperer ?\n","\n","Pour pouvoir avoir une idee de l’architecture du site web que l’on etudie on peut utiliser une fonctionnalité tres utile disponible dans tous les navigateurs : Inspecter l’element."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bnZRtcamvi3I"},"source":["![image](https://user-images.githubusercontent.com/123748165/224308453-e6a3ac7f-2d9e-4b14-a905-6bdff6fb1a2a.png)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8kpxC8CWrGdR"},"outputs":[],"source":["website=\"https://larevueia.fr/\""]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"pErUCfBUAtWx"},"source":["## **TO DO**\n","- ajoutez l'URL du site Web à votre driver avec la methode get ()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9keh-cumokx1"},"outputs":[],"source":["#fill_here"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xgdad_YAvpa5"},"source":["- Dans notre cas, on se rend vite compte que les articles sont répartis en categories et que chaque categorie contient plusieurs pages. \n","\n","- On peut donc commencer par recuperer tout d’abord les liens des articles pour ensuite pouvoir recuperer les donnees article par article.\n","\n","- Pour ce faire, on va tout d’abord creer une liste contenant l’ensemble des categories de notre site :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_icARXLVqwHZ"},"outputs":[],"source":["categories = ['ethique', 'nlp', 'evenements', 'ml-dl', 'data-science', 'vision']"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6IMV48kTvs_h"},"source":["On peut donc facilement explorer le site en parcourant les liens des pages des differentes categories :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aCgh3rLOqyJO"},"outputs":[],"source":["articles_links = []\n","for category in categories:\n","  category_link = website + category\n","  #ajoutez l'URL category_link à votre driver avec la méthode get ()\n","  #fill_here\n","  # Trouver les liens href des articles dans chaque catégorie à l'aide de la méthode driver.find_elements(inspecter la page source)\n","  href_links = #fill_here\n","  for href_link in href_links:\n","    articles_links.append([href_link.get_attribute('href'), category])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2opPYt_8v2vf"},"source":["- Pour expliquer plus precisement ce que fait le code ci-dessus, on peut dire que l’on parcourt tout d’abord la liste des categories, pour chaque categorie on cree une variable, qui contient son URL.\n","\n","- Puis on ouvre la page avec la fonction get() de notre driver.\n","\n","- On utilise ensuite la fonction find_elements_by_xpath() pour rechercher les liens href se trouvant à l’interieur de balise h2 (ceci découle directement de notre analyse du site avec la fonctionnalite Inspecter l’element de notre navigateur, vous pouvez copier le xpath directement sur le navigateur).\n","\n","- Finalement, on ne recupere que le lien de l’article avec la fonction get_attribute(), et on ajoute donc à notre liste le lien de l’article ainsi que la categorie à laquelle il correspond (ce qui peut etre utile plus tard pour faire du NLP avec un modèle de classification par exemple).\n","\n","- On pourrait aller plus loin et rajouter une deuxième boucle pour incrementer à chaque fois le nombre de pages afin de parcourir tous les articles de chaque categorie, et pas seulement ceux presents sur la premiere page."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0XZyDOujwgjv"},"source":["# **Web Scraping du contenu des articles avec Selenium**\n","\n","- Une fois l’ensemble des liens d’articles recuperes, la prochaine etape est d’analyser le contenu de la page pour chaque article. \n","\n","- Encore une fois, on utilise la fonctionnalité Inspecter l’element, afin d’identifier la structure de la page et de retrouver les elements à scraper.\n","\n","- On se rend vite compte que dans notre cas le contenu de l’article est stocké dans une classe nommée article-post. On va donc recuperer le contenu texte de cette classe et tout stocker dans un data frame avec Pandas :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s-dr087cq0QQ"},"outputs":[],"source":["df_articles = pd.DataFrame()\n","for article_link in articles_links:\n","  #Ajouter les liens des articles article_link avec la méthode get() \n","  #fill_here\n","  #Trouver le contenu d'article_url (ce qu'on veut extraire , dans notre cas le text de l'article) avec driver.find_elements()\n","  content_scrap = #fill_here\n","  if (len(content_scrap) != 0):\n","    content = content_scrap[0].text\n","  article = {'category' : article_link[1], 'content' : content}\n","  df_article = pd.DataFrame(article, index = [0])\n","  df_articles = df_articles.append(df_article, ignore_index=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"i-lR_5EfxE_I"},"source":["On se retrouve donc avec un data frame contenant nos articles, avec à chaque fois la catégorie à laquelle il correspond. On peut finalement transformer ce data frame en fichier csv, afin de stocker les donnees :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQaeCRBzq2WQ"},"outputs":[],"source":["df_articles.to_csv('la_revue_ia_articles.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t9gbrxZDr9pq"},"outputs":[],"source":["df_articles"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.0"},"vscode":{"interpreter":{"hash":"ab1d0033c7335c0fe0bc7bbdf0206b9f203ed6908047dbf4ec92ecd764faa004"}}},"nbformat":4,"nbformat_minor":0}
